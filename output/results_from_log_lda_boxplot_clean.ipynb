{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6261cb67-945d-405f-af59-2288efb49002",
   "metadata": {},
   "source": [
    "# Statistical Analysis of Latent Space and Segmentation Metrics\n",
    "\n",
    "This notebook performs a full statistical analysis of the latent space and segmentation metrics produced by the model trained on \"Normal\" muscles.\n",
    "\n",
    "It includes:\n",
    "\n",
    "* Extraction of latent codes from log files\n",
    "* PCA and MANOVA to evaluate group separation\n",
    "* LDA visualization and Fisher scores\n",
    "* Computation of segmentation metrics (DSC, ASD, HSD95, volume errors, etc.)\n",
    "* Group comparison (healthy vs sarcopenia patients) using:\n",
    "  * Shapiro–Wilk normality tests\n",
    "  * Mann–Whitney U tests\n",
    "  * Linear Mixed-Effects Models (LME)\n",
    "* ROC curve analysis to study classification performance\n",
    "* Threshold extraction for DSC to separate sarcopenia patients from healthy subjects\n",
    "* Boxplots of metrics per fold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845975c-7cb7-45b3-a938-0d761bda796e",
   "metadata": {},
   "source": [
    "# ⚙️ How to use this notebook\n",
    "\n",
    "1. Set the muscle you want to analyze, and specify the number of folds if doing cross-validation:\n",
    "   - `muscle` corresponds to the folder name inside `casename_files/DIASEM/`.\n",
    "   - `n_folds` can be set to 1 for a single dataset or higher for cross-validation.\n",
    "\n",
    "2. Place your log files in the expected folder structure:\n",
    "   - Logs should come directly from `eval.py` and be located in `./output/` (one log per test/train set).\n",
    "\n",
    "3. Create subject list files:\n",
    "   - One subject per line in text files.\n",
    "   - Optional: assign groups manually, or the notebook will categorize automatically based on filename patterns.\n",
    "\n",
    "4. Run all cells.\n",
    "\n",
    "5. Outputs generated:\n",
    "   - `fisher_scores_and_metrics_<muscle>.csv`\n",
    "   - LDA plots\n",
    "   - Boxplots for all metrics (PDF)\n",
    "   - ROC curve figure\n",
    "   - Computed DSC threshold for classifying sarcopenia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021bdec-3689-410b-8fad-d1252b90eec4",
   "metadata": {},
   "source": [
    "# Note for clinicians\n",
    "\n",
    "This notebook provides a quantitative analysis of segmentation quality and group separation.\n",
    "\n",
    "You can use it to:\n",
    "\n",
    "* Visualize how well the model distinguishes healthy vs sarcopenic subjects\n",
    "* Study segmentation accuracy for clinical quality assurance\n",
    "* Extract thresholds (e.g., DSC) to support diagnosis\n",
    "\n",
    "No coding knowledge is required — simply update the text files containing subject lists and run the notebook.\n",
    "\n",
    "⚠️ Note: This analysis provides quantitative metrics and thresholds, but it should not be used as a standalone diagnostic tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102edd39-3c21-4f75-9b86-bf08054a1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English comments inside code\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "\n",
    "from scipy.stats import shapiro, mannwhitneyu, combine_pvalues\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "# Optional: adjust plotting style (clinician-friendly)\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c407e47-e5a3-4dee-b864-ebb96758aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONFIG ----------\n",
    "# Edit these variables for your run\n",
    "muscle = 'RF'               # folder name under ../casename_files/DIASEM/\n",
    "n_folds = 1                 # 1 = single fold (no CV), >1 = cross-validation\n",
    "output_csv = f\"fisher_scores_and_metrics_{muscle}.csv\"\n",
    "output_dir = f\"plots_{muscle}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Log file name patterns (adjust if your logs are named differently)\n",
    "log_patterns = {\n",
    "    'test': './DIASEM_{muscle}_set{fold}_NO_MISMATCH_registered_turned_1500_eval_ax2_x1_healthy/log.txt',\n",
    "    'sarcopenia': './DIASEM_{muscle}_set{fold}_NO_MISMATCH_registered_turned_1500_eval_ax2_x1_sarcopenia/log.txt',\n",
    "    'train': './DIASEM_{muscle}_set{fold}_NO_MISMATCH_registered_turned_1500_eval_ax2_x1_train/log.txt'\n",
    "}\n",
    "# Paths to subject lists\n",
    "subject_lists_base = f'../casename_files/DIASEM/{muscle}'\n",
    "path_sarcopenia = os.path.join(subject_lists_base, 'sarcopenia_subj_DIASEM.txt')\n",
    "# train/test files are constructed per fold: train_cases_{fold}.txt, test_cases_{fold}.txt\n",
    "# ----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9270b591-f7aa-4429-9b2e-a20af198ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions: parsing, converting string tensors, categorization\n",
    "\n",
    "def safe_readlines(path):\n",
    "    \"\"\"Return lines of file, or empty list if not found.\"\"\"\n",
    "    if not os.path.isfile(path):\n",
    "        return []\n",
    "    with open(path, 'r') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def load_subjects(file_path, group=None):\n",
    "    \"\"\"Load a subject text file (one per line). If group provided, return dataframe with Group.\"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        return pd.DataFrame(columns=['Subject','Group']) if group else []\n",
    "    with open(file_path, 'r') as f:\n",
    "        subjects = [line.strip().lower() for line in f if line.strip()]\n",
    "    if group:\n",
    "        return pd.DataFrame({'Subject': subjects, 'Group': group})\n",
    "    return subjects\n",
    "\n",
    "def categorize_subjects(subjects):\n",
    "    \"\"\"Simple rule: 'sujet...' -> healthy_subjects, else patients_wo_sarcopenia.\"\"\"\n",
    "    categorized = []\n",
    "    for subj in subjects:\n",
    "        if subj.startswith('sujet'):\n",
    "            categorized.append({'Subject': subj, 'Group': 'healthy_subjects'})\n",
    "        else:\n",
    "            categorized.append({'Subject': subj, 'Group': 'patients_wo_sarcopenia'})\n",
    "    return pd.DataFrame(categorized)\n",
    "\n",
    "def convert_tensor_string_to_array(tensor_str):\n",
    "    \"\"\"Parse verbose tensor string saved in logs to numpy array.\n",
    "       Many formats are possible; try common ones, else return None.\"\"\"\n",
    "    if not isinstance(tensor_str, str) or len(tensor_str.strip()) == 0:\n",
    "        return None\n",
    "    # remove common prefixes\n",
    "    s = re.sub(r'Parameter containing:', '', tensor_str).strip()\n",
    "    s = s.replace(\"tensor(\", \"\").split(\", device=\")[0]\n",
    "    # ensure parentheses balanced/convertible to Python literal\n",
    "    try:\n",
    "        arr = literal_eval(s)\n",
    "        return np.array(arr)\n",
    "    except Exception:\n",
    "        # fallback: extract floats from string\n",
    "        nums = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n",
    "        if len(nums) == 0:\n",
    "            return None\n",
    "        return np.array([float(x) for x in nums])\n",
    "\n",
    "def parse_log_file(path):\n",
    "    \"\"\"Parse a log file produced by eval.py and return DataFrame with metrics and z-latents per subject.\n",
    "       The parser is tolerant: if patterns differ slightly, it tries best-effort extraction.\n",
    "    \"\"\"\n",
    "    lines = safe_readlines(path)\n",
    "    if not lines:\n",
    "        return pd.DataFrame()\n",
    "    parsed = []\n",
    "    i = 0\n",
    "    batch_cases = []\n",
    "    z_latents = \"\"\n",
    "    stats = {}\n",
    "    # regex to capture metrics line: adapt to your exact print format if necessary\n",
    "    stats_pattern = re.compile(r\"ASD: *([-\\d.]+).*?HSD: *([-\\d.]+).*?HSD95: *([-\\d.]+).*?DSC: *([-\\d.]+).*?err_vol_cm_3: *([-\\d.]+).*?err_vol_percent: *([-\\d.]+)\", re.IGNORECASE)\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        if \"Batch cases:\" in line:\n",
    "            batch_cases = [x.strip().lower() for x in line.split(\"Batch cases:\")[-1].strip().split(',') if x.strip()]\n",
    "        elif \"z:\" in line and batch_cases:\n",
    "            # collect subsequent lines containing tensor data (best effort)\n",
    "            tensor_parts = [line.split('z:')[-1].strip()]\n",
    "            i += 1\n",
    "            while i < len(lines) and (lines[i].strip().startswith('tensor') or 'Parameter containing' in lines[i] or re.search(r'[\\[\\]\\d\\.\\-]', lines[i])):\n",
    "                tensor_parts.append(lines[i].strip())\n",
    "                i += 1\n",
    "            z_latents = ' '.join(tensor_parts)\n",
    "            continue\n",
    "        elif batch_cases and (\"Batch ASD:\" in line or \"ASD:\" in line):\n",
    "            m = stats_pattern.search(line)\n",
    "            if m:\n",
    "                stats = {\n",
    "                    'ASD': float(m.group(1)),\n",
    "                    'HSD': float(m.group(2)),\n",
    "                    'HSD95': float(m.group(3)),\n",
    "                    'DSC': float(m.group(4)),\n",
    "                    'Err_Vol_CM3': float(m.group(5)),\n",
    "                    'Err_Vol_Percent': float(m.group(6))\n",
    "                }\n",
    "        if batch_cases and stats:\n",
    "            for case in batch_cases:\n",
    "                parsed.append((case.strip().lower(), z_latents, stats.get('ASD'), stats.get('HSD'),\n",
    "                               stats.get('HSD95'), stats.get('DSC'), stats.get('Err_Vol_CM3'), stats.get('Err_Vol_Percent')))\n",
    "            batch_cases, z_latents, stats = [], \"\", {}\n",
    "        i += 1\n",
    "    cols = ['Subject','z_latents_batch','ASD','HSD','HSD95','DSC','Err_Vol_CM3','Err_Vol_Percent']\n",
    "    return pd.DataFrame(parsed, columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b70ebc46-eacf-4bb6-89df-e582dc8895d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fold(fold, muscle, log_patterns, subject_lists_base):\n",
    "    \"\"\"\n",
    "    Process a single fold: load subject lists, parse logs, compute PCA/LDA/Fisher/MANOVA,\n",
    "    and return dicts/dataframes with results and metrics for boxplots/ROC.\n",
    "    \"\"\"\n",
    "    print(f\"Processing fold {fold} ...\")\n",
    "    # build paths\n",
    "    path_train = os.path.join(subject_lists_base, f\"train_cases_{fold}.txt\")\n",
    "    path_test = os.path.join(subject_lists_base, f\"test_cases_{fold}.txt\")\n",
    "    path_sarc = os.path.join(subject_lists_base, 'sarcopenia_subj_DIASEM.txt')\n",
    "\n",
    "    # load lists\n",
    "    sarc_df = load_subjects(path_sarc, 'patient_with_sarcopenia')\n",
    "    train_df = categorize_subjects(load_subjects(path_train))\n",
    "    test_df = categorize_subjects(load_subjects(path_test))\n",
    "\n",
    "    # parse logs\n",
    "    logs = {}\n",
    "    for key, pattern in log_patterns.items():\n",
    "        log_path = pattern.format(muscle=muscle, fold=fold)\n",
    "        logs[key] = parse_log_file(log_path)\n",
    "        if not logs[key].empty:\n",
    "            logs[key]['Subject'] = logs[key]['Subject'].str.strip(\"[]\").str.replace(\"'\", \"\")\n",
    "            logs[key]['z_latents_batch'] = logs[key]['z_latents_batch'].apply(convert_tensor_string_to_array)\n",
    "\n",
    "    # merge subject group info\n",
    "    if not logs.get('test', pd.DataFrame()).empty:\n",
    "        logs['test'] = logs['test'].merge(test_df, on='Subject', how='left').assign(Set='Test')\n",
    "    if not logs.get('sarcopenia', pd.DataFrame()).empty:\n",
    "        logs['sarcopenia'] = logs['sarcopenia'].merge(sarc_df, on='Subject', how='left').assign(Set='Test')\n",
    "    if not logs.get('train', pd.DataFrame()).empty:\n",
    "        logs['train'] = logs['train'].merge(train_df, on='Subject', how='left').assign(Set='Train')\n",
    "\n",
    "    # combine for latent analysis\n",
    "    combined_df = pd.concat([df for df in [logs.get('test', pd.DataFrame()), logs.get('sarcopenia', pd.DataFrame()), logs.get('train', pd.DataFrame())] if not df.empty], ignore_index=True)\n",
    "    # ensure we have latent vectors\n",
    "    if combined_df.empty:\n",
    "        print(f\"No data for fold {fold}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    shapes = combined_df['z_latents_batch'].apply(lambda x: None if x is None else x.shape)\n",
    "    if shapes.dropna().empty:\n",
    "        print(f\"No latent arrays parsed for fold {fold}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    most_common_shape = Counter(shapes.dropna()).most_common(1)[0][0]\n",
    "    df_filtered = combined_df[combined_df['z_latents_batch'].apply(lambda x: x is not None and x.shape == most_common_shape)]\n",
    "    if df_filtered.empty:\n",
    "        print(f\"No filtered latent arrays for fold {fold}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Build X and y for latent analysis\n",
    "    X = np.stack(df_filtered['z_latents_batch'].values)\n",
    "    if X.ndim == 3:\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "    y = df_filtered['Group'].values\n",
    "\n",
    "    # PCA\n",
    "    n_components = min(64, X.shape[1], X.shape[0])\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    pca_cols = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "    df_latents = pd.DataFrame(X_pca, columns=pca_cols)\n",
    "    df_latents[\"Group\"] = y\n",
    "\n",
    "    # MANOVA (best-effort)\n",
    "    try:\n",
    "        formula = \" + \".join(pca_cols) + \" ~ Group\"\n",
    "        maov = MANOVA.from_formula(formula, data=df_latents)\n",
    "        manova_res = maov.mv_test()\n",
    "        p_value = manova_res.results['Group']['stat']['Pr > F'][\"Wilks' lambda\"]\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"MANOVA failed on fold {fold}: {e}\")\n",
    "        p_value = np.nan\n",
    "\n",
    "    # LDA + Fisher\n",
    "    try:\n",
    "        lda = LDA(n_components=2)\n",
    "        z_lda = lda.fit_transform(X, y)\n",
    "        # compute multivariate fisher score on lda coordinates\n",
    "        classes = np.unique(y)\n",
    "        overall_mean = np.mean(z_lda, axis=0)\n",
    "        S_B, S_W = np.zeros((2,2)), np.zeros((2,2))\n",
    "        for cls in classes:\n",
    "            X_c = z_lda[y == cls]\n",
    "            if X_c.shape[0] <= 1:\n",
    "                continue\n",
    "            mean_c = np.mean(X_c, axis=0)\n",
    "            n_c = X_c.shape[0]\n",
    "            mean_diff = (mean_c - overall_mean).reshape(-1, 1)\n",
    "            S_B += n_c * (mean_diff @ mean_diff.T)\n",
    "            S_W += np.cov(X_c, rowvar=False) * (n_c - 1)\n",
    "        fisher_score_multi = np.trace(S_B) / (np.trace(S_W) if np.trace(S_W) != 0 else np.nan)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"LDA/Fisher failed on fold {fold}: {e}\")\n",
    "        fisher_score_multi = np.nan\n",
    "        z_lda = None\n",
    "\n",
    "    # collect test metrics for boxplots/ROC\n",
    "    df_metrics_list = []\n",
    "    for name in ['test','sarcopenia']:\n",
    "        if not logs.get(name, pd.DataFrame()).empty:\n",
    "            dfm = logs[name].copy()\n",
    "            # ensure DSC numeric\n",
    "            for col in ['DSC','ASD','HSD','HSD95','Err_Vol_CM3','Err_Vol_Percent']:\n",
    "                if col in dfm.columns:\n",
    "                    dfm[col] = pd.to_numeric(dfm[col], errors='coerce')\n",
    "            dfm['fold'] = fold\n",
    "            dfm['source'] = f\"Fold_{fold}_Test_Set_{'healthy' if name=='test' else 'sarcopenia'}\"\n",
    "            df_metrics_list.append(dfm)\n",
    "    df_metrics = pd.concat(df_metrics_list, ignore_index=True) if df_metrics_list else pd.DataFrame()\n",
    "\n",
    "    return {\n",
    "        'fold': fold,\n",
    "        'df_latents': df_latents,\n",
    "        'p_value_manova': p_value,\n",
    "        'fisher_score': fisher_score_multi,\n",
    "        'z_lda': z_lda,\n",
    "        'df_metrics': df_metrics,\n",
    "        'df_filtered': df_filtered  # useful for plotting LDA points\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa856b1d-8a4f-41df-a16d-0d1190234f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1 ...\n",
      "Processing done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>fisher_score</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.478506</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold  fisher_score  p_value\n",
       "0     1      0.478506      NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run processing for all folds\n",
    "fold_results = []\n",
    "df_metrics_all_list = []\n",
    "\n",
    "for fold in range(1, n_folds + 1):\n",
    "    res = process_fold(fold, muscle, log_patterns, subject_lists_base)\n",
    "    if res:\n",
    "        fold_results.append(res)\n",
    "        if not res['df_metrics'].empty:\n",
    "            df_metrics_all_list.append(res['df_metrics'])\n",
    "\n",
    "# collect fisher + p-values summary\n",
    "df_results = pd.DataFrame([{'fold': r['fold'], 'fisher_score': r['fisher_score'], 'p_value': r['p_value_manova']} for r in fold_results])\n",
    "if df_metrics_all_list:\n",
    "    df_combined = pd.concat(df_metrics_all_list, ignore_index=True)\n",
    "else:\n",
    "    df_combined = pd.DataFrame()\n",
    "\n",
    "print(\"Processing done.\")\n",
    "display(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2985f12-f533-4f1e-b8b7-bfc509c79e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d8d0b-2380-4c73-8167-0c340e3ba4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
