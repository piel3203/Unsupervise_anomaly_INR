# Split in training/validation happens internally.
train_casefile: train_cases_1.txt

# If None, nothing is written to filesystem during training (useful for debugging).
model_name: "DIASEM_RF_set1_NO_MISMATCH_registered_turned_CLINICAL_INFOS_BASELINE_Z64"

opt_rot: False
opt_vect: False
selected_params: ['ID_subject']
#selected_params: ['ID_subject','Sexe', 'Age', 'Volume_RF']
# 0 -- AutoDecoder (AD), 1 -- ReconNet (RN)CORO_SIP
task_type: 0
# For RN: crop size for training. If 0, disabled.
crop_size: 128
# For AD/RN: fraction of training data used for validation
val_fraction: 0.10
# For AD/RN: thin out volumes by keeping every x slice. Should be at least 2.
slice_step_size: 1
# For AD/RN: axis for volume thinning in LPS coordinate system: sagittal, coronal, axial.
slice_step_axis: 2
# For AD/RN: whether to use averaged thick slices instead of exact thin slices.
use_thick_slices: False
# For AD task: latent dimension
latent_dim: 64
# For AD: number of layers in OccupancyPredictor
op_num_layers: 8
# For AD: layers with coordinates in OccupancyPredictor
op_coord_layers: [0, 4]

batch_size_train: 1
batch_size_val: 1
# For AD: number of points per example per dim for training. If -1, use the whole volume.
#num_points_per_example_per_dim_train: -1
num_points_per_example_per_dim_train: -1
# For AD: latent regularization weight. If 0, disabled.
lat_reg_lambda: 1.0e-4
# Scaled with batch size
learning_rate: 1.0e-4
# For AD: not scaled with batch size; only used for training.
learning_rate_lat: 1.0e-3

#new lr for the optim of the rotations
learning_rate_rot: 1.0e-4

num_epochs: 2500
# Log to tensorboard every x epochs (train and val)
log_epoch_count: 2
# Store model checkpoint every x epochs
checkpoint_epoch_count: 50
max_num_checkpoints: 25

num_workers: 1
